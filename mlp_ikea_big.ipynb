{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nn(frame_size, n_hid, lr):\n",
    "    model = Sequential() # The Keras Sequential model is a linear stack of layers.\n",
    "    model.add(Dense(n_hid, kernel_initializer='uniform', input_dim=frame_size)) # Dense layer\n",
    "    model.add(Activation('tanh')) # Activation layer\n",
    "    model.add(Dropout(0.5)) # Dropout layer\n",
    "    model.add(Dense(n_hid, kernel_initializer='uniform')) # Another dense layer\n",
    "    model.add(Activation('tanh')) # Another activation layer\n",
    "    model.add(Dropout(0.5)) # Another dropout layer\n",
    "    model.add(Dense(12, init='uniform')) # Last dense layer\n",
    "    model.add(Activation('softmax')) # Softmax activation at the end\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True) # Using Nesterov momentum\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['categorical_accuracy']) # Using logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    file = pd.read_csv('embed_ikea_intent.csv')\n",
    "    features = file.drop(['index', 'intent'], axis=1).values\n",
    "    labels = file['intent'].values\n",
    "    n_frames, frame_size = features.shape\n",
    "    return features, labels, n_frames, frame_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, n_frames, frame_size = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "label_dictionary = {}\n",
    "label_int_value = 0\n",
    "for label in unique_labels:\n",
    "    if not label in label_dictionary:\n",
    "        label_dictionary[label]=label_int_value\n",
    "        label_int_value+=1\n",
    "new_labels = []\n",
    "for label in labels:\n",
    "    new_labels.append(label_dictionary[label])\n",
    "new_labels = np.array(new_labels)\n",
    "records = []\n",
    "avg_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hidden nodes =  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheshank.k\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Train on 3762 samples, validate on 418 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.0080 - categorical_accuracy: 0.2730 - val_loss: 2.1543 - val_categorical_accuracy: 0.5383\n",
      "Epoch 2/20\n",
      " - 1s - loss: 1.7525 - categorical_accuracy: 0.3884 - val_loss: 2.0175 - val_categorical_accuracy: 0.4330\n",
      "Epoch 3/20\n",
      " - 1s - loss: 1.5338 - categorical_accuracy: 0.4747 - val_loss: 2.0014 - val_categorical_accuracy: 0.5789\n",
      "Epoch 4/20\n",
      " - 1s - loss: 1.3534 - categorical_accuracy: 0.5473 - val_loss: 2.0914 - val_categorical_accuracy: 0.5789\n",
      "Epoch 5/20\n",
      " - 1s - loss: 1.2981 - categorical_accuracy: 0.5662 - val_loss: 1.9512 - val_categorical_accuracy: 0.6196\n",
      "Epoch 6/20\n",
      " - 1s - loss: 1.2261 - categorical_accuracy: 0.5984 - val_loss: 2.3520 - val_categorical_accuracy: 0.5885\n",
      "Epoch 7/20\n",
      " - 1s - loss: 1.2322 - categorical_accuracy: 0.5909 - val_loss: 2.2281 - val_categorical_accuracy: 0.6005\n",
      "Epoch 8/20\n",
      " - 1s - loss: 1.2002 - categorical_accuracy: 0.6063 - val_loss: 2.2343 - val_categorical_accuracy: 0.6172\n",
      "Epoch 9/20\n",
      " - 1s - loss: 1.2105 - categorical_accuracy: 0.5976 - val_loss: 2.0488 - val_categorical_accuracy: 0.6172\n",
      "Epoch 10/20\n",
      " - 1s - loss: 1.2687 - categorical_accuracy: 0.5965 - val_loss: 2.0242 - val_categorical_accuracy: 0.6627\n",
      "Epoch 11/20\n",
      " - 1s - loss: 1.2600 - categorical_accuracy: 0.6103 - val_loss: 2.1268 - val_categorical_accuracy: 0.5933\n",
      "Epoch 12/20\n",
      " - 1s - loss: 1.2778 - categorical_accuracy: 0.6172 - val_loss: 2.2828 - val_categorical_accuracy: 0.6077\n",
      "Epoch 13/20\n",
      " - 1s - loss: 1.3414 - categorical_accuracy: 0.6082 - val_loss: 2.2947 - val_categorical_accuracy: 0.6340\n",
      "Epoch 14/20\n",
      " - 1s - loss: 1.4837 - categorical_accuracy: 0.5821 - val_loss: 3.1263 - val_categorical_accuracy: 0.4402\n",
      "Epoch 15/20\n",
      " - 1s - loss: 1.7308 - categorical_accuracy: 0.5688 - val_loss: 3.0501 - val_categorical_accuracy: 0.6411\n",
      "Epoch 16/20\n",
      " - 1s - loss: 2.0568 - categorical_accuracy: 0.5556 - val_loss: 4.2111 - val_categorical_accuracy: 0.4928\n",
      "Epoch 17/20\n",
      " - 1s - loss: 2.5319 - categorical_accuracy: 0.5369 - val_loss: 3.4693 - val_categorical_accuracy: 0.6388\n",
      "Epoch 18/20\n",
      " - 1s - loss: 3.5368 - categorical_accuracy: 0.4825 - val_loss: 3.8801 - val_categorical_accuracy: 0.5789\n",
      "Epoch 19/20\n",
      " - 1s - loss: 4.9541 - categorical_accuracy: 0.4274 - val_loss: 5.1017 - val_categorical_accuracy: 0.5502\n",
      "Epoch 20/20\n",
      " - 1s - loss: 6.2581 - categorical_accuracy: 0.3907 - val_loss: 4.1840 - val_categorical_accuracy: 0.6435\n",
      "End training\n",
      "accuracy =  0.4 time_taken =  21.898\n",
      "Start training\n",
      "Train on 3765 samples, validate on 419 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 7.3831 - categorical_accuracy: 0.3644 - val_loss: 5.6884 - val_categorical_accuracy: 0.6062\n",
      "Epoch 2/20\n",
      " - 1s - loss: 7.8434 - categorical_accuracy: 0.3631 - val_loss: 5.5191 - val_categorical_accuracy: 0.5752\n",
      "Epoch 3/20\n",
      " - 1s - loss: 8.0759 - categorical_accuracy: 0.3628 - val_loss: 6.8533 - val_categorical_accuracy: 0.4487\n",
      "Epoch 4/20\n",
      " - 1s - loss: 8.2498 - categorical_accuracy: 0.3556 - val_loss: 6.5791 - val_categorical_accuracy: 0.5084\n",
      "Epoch 5/20\n",
      " - 1s - loss: 8.5141 - categorical_accuracy: 0.3559 - val_loss: 8.6338 - val_categorical_accuracy: 0.4010\n",
      "Epoch 6/20\n",
      " - 1s - loss: 8.8163 - categorical_accuracy: 0.3477 - val_loss: 5.5491 - val_categorical_accuracy: 0.6062\n",
      "Epoch 7/20\n",
      " - 1s - loss: 9.2299 - categorical_accuracy: 0.3389 - val_loss: 4.6761 - val_categorical_accuracy: 0.5800\n",
      "Epoch 8/20\n",
      " - 1s - loss: 9.3271 - categorical_accuracy: 0.3432 - val_loss: 6.4736 - val_categorical_accuracy: 0.5155\n",
      "Epoch 9/20\n",
      " - 1s - loss: 9.5037 - categorical_accuracy: 0.3331 - val_loss: 5.8189 - val_categorical_accuracy: 0.6014\n",
      "Epoch 10/20\n",
      " - 1s - loss: 9.7284 - categorical_accuracy: 0.3256 - val_loss: 7.0556 - val_categorical_accuracy: 0.5346\n",
      "Epoch 11/20\n",
      " - 1s - loss: 9.5504 - categorical_accuracy: 0.3325 - val_loss: 5.7232 - val_categorical_accuracy: 0.6062\n",
      "Epoch 12/20\n",
      " - 1s - loss: 9.7280 - categorical_accuracy: 0.3442 - val_loss: 6.0281 - val_categorical_accuracy: 0.6014\n",
      "Epoch 13/20\n",
      " - 1s - loss: 9.5243 - categorical_accuracy: 0.3673 - val_loss: 6.3253 - val_categorical_accuracy: 0.5871\n",
      "Epoch 14/20\n",
      " - 1s - loss: 9.5864 - categorical_accuracy: 0.3718 - val_loss: 7.7211 - val_categorical_accuracy: 0.4845\n",
      "Epoch 15/20\n",
      " - 1s - loss: 9.6378 - categorical_accuracy: 0.3618 - val_loss: 7.3296 - val_categorical_accuracy: 0.4988\n",
      "Epoch 16/20\n",
      " - 1s - loss: 9.3163 - categorical_accuracy: 0.3928 - val_loss: 6.1653 - val_categorical_accuracy: 0.5871\n",
      "Epoch 17/20\n",
      " - 1s - loss: 9.6496 - categorical_accuracy: 0.3647 - val_loss: 6.5148 - val_categorical_accuracy: 0.5632\n",
      "Epoch 18/20\n",
      " - 1s - loss: 9.5761 - categorical_accuracy: 0.3602 - val_loss: 7.4914 - val_categorical_accuracy: 0.5203\n",
      "Epoch 19/20\n",
      " - 1s - loss: 9.4183 - categorical_accuracy: 0.3761 - val_loss: 7.1936 - val_categorical_accuracy: 0.5203\n",
      "Epoch 20/20\n",
      " - 1s - loss: 9.9373 - categorical_accuracy: 0.3477 - val_loss: 6.4257 - val_categorical_accuracy: 0.5823\n",
      "End training\n",
      "accuracy =  0.491 time_taken =  17.082\n",
      "Start training\n",
      "Train on 3765 samples, validate on 419 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 9.9765 - categorical_accuracy: 0.3413 - val_loss: 6.5913 - val_categorical_accuracy: 0.5704\n",
      "Epoch 2/20\n",
      " - 1s - loss: 10.1355 - categorical_accuracy: 0.3299 - val_loss: 6.6673 - val_categorical_accuracy: 0.5704\n",
      "Epoch 3/20\n",
      " - 1s - loss: 10.0243 - categorical_accuracy: 0.3299 - val_loss: 6.2449 - val_categorical_accuracy: 0.5895\n",
      "Epoch 4/20\n",
      " - 1s - loss: 10.3406 - categorical_accuracy: 0.3145 - val_loss: 6.1639 - val_categorical_accuracy: 0.6038\n",
      "Epoch 5/20\n",
      " - 1s - loss: 10.5253 - categorical_accuracy: 0.3116 - val_loss: 6.8364 - val_categorical_accuracy: 0.5394\n",
      "Epoch 6/20\n",
      " - 1s - loss: 10.1494 - categorical_accuracy: 0.3288 - val_loss: 8.8304 - val_categorical_accuracy: 0.3365\n",
      "Epoch 7/20\n",
      " - 1s - loss: 10.1564 - categorical_accuracy: 0.3301 - val_loss: 7.1021 - val_categorical_accuracy: 0.4224\n",
      "Epoch 8/20\n",
      " - 1s - loss: 9.9987 - categorical_accuracy: 0.3453 - val_loss: 6.4535 - val_categorical_accuracy: 0.5036\n",
      "Epoch 9/20\n",
      " - 1s - loss: 9.9489 - categorical_accuracy: 0.3570 - val_loss: 6.7483 - val_categorical_accuracy: 0.5680\n",
      "Epoch 10/20\n",
      " - 1s - loss: 10.6664 - categorical_accuracy: 0.3108 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "Epoch 11/20\n",
      " - 1s - loss: 10.6998 - categorical_accuracy: 0.3118 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 12/20\n",
      " - 1s - loss: 11.1738 - categorical_accuracy: 0.2911 - val_loss: 10.0802 - val_categorical_accuracy: 0.3747\n",
      "Epoch 13/20\n",
      " - 1s - loss: 10.7539 - categorical_accuracy: 0.3089 - val_loss: 7.3358 - val_categorical_accuracy: 0.4272\n",
      "Epoch 14/20\n",
      " - 1s - loss: 10.5927 - categorical_accuracy: 0.3208 - val_loss: 5.8641 - val_categorical_accuracy: 0.5967\n",
      "Epoch 15/20\n",
      " - 1s - loss: 10.0853 - categorical_accuracy: 0.3517 - val_loss: 6.6522 - val_categorical_accuracy: 0.5704\n",
      "Epoch 16/20\n",
      " - 1s - loss: 10.9893 - categorical_accuracy: 0.2855 - val_loss: 5.7099 - val_categorical_accuracy: 0.5871\n",
      "Epoch 17/20\n",
      " - 1s - loss: 10.2967 - categorical_accuracy: 0.3317 - val_loss: 6.6938 - val_categorical_accuracy: 0.5728\n",
      "Epoch 18/20\n",
      " - 1s - loss: 10.5225 - categorical_accuracy: 0.3179 - val_loss: 6.2454 - val_categorical_accuracy: 0.5322\n",
      "Epoch 19/20\n",
      " - 1s - loss: 10.6241 - categorical_accuracy: 0.3182 - val_loss: 10.1236 - val_categorical_accuracy: 0.3652\n",
      "Epoch 20/20\n",
      " - 1s - loss: 10.7258 - categorical_accuracy: 0.3158 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "End training\n",
      "accuracy =  0.256 time_taken =  17.429\n",
      "Start training\n",
      "Train on 3768 samples, validate on 419 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 11.0610 - categorical_accuracy: 0.2946 - val_loss: 9.6288 - val_categorical_accuracy: 0.3628\n",
      "Epoch 2/20\n",
      " - 1s - loss: 10.9180 - categorical_accuracy: 0.3002 - val_loss: 6.4979 - val_categorical_accuracy: 0.5036\n",
      "Epoch 3/20\n",
      " - 1s - loss: 10.8679 - categorical_accuracy: 0.3033 - val_loss: 8.7846 - val_categorical_accuracy: 0.3819\n",
      "Epoch 4/20\n",
      " - 1s - loss: 10.5731 - categorical_accuracy: 0.3267 - val_loss: 6.7803 - val_categorical_accuracy: 0.5680\n",
      "Epoch 5/20\n",
      " - 1s - loss: 10.6373 - categorical_accuracy: 0.3246 - val_loss: 11.2217 - val_categorical_accuracy: 0.3007\n",
      "Epoch 6/20\n",
      " - 1s - loss: 10.7664 - categorical_accuracy: 0.3156 - val_loss: 9.6890 - val_categorical_accuracy: 0.3962\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 10.5424 - categorical_accuracy: 0.3248 - val_loss: 10.4953 - val_categorical_accuracy: 0.3174\n",
      "Epoch 8/20\n",
      " - 1s - loss: 11.0834 - categorical_accuracy: 0.2895 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 9/20\n",
      " - 1s - loss: 11.2327 - categorical_accuracy: 0.2848 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "Epoch 10/20\n",
      " - 1s - loss: 10.9680 - categorical_accuracy: 0.2943 - val_loss: 5.7942 - val_categorical_accuracy: 0.5943\n",
      "Epoch 11/20\n",
      " - 1s - loss: 10.7138 - categorical_accuracy: 0.3169 - val_loss: 10.1951 - val_categorical_accuracy: 0.3628\n",
      "Epoch 12/20\n",
      " - 1s - loss: 10.6569 - categorical_accuracy: 0.3110 - val_loss: 10.6667 - val_categorical_accuracy: 0.3031\n",
      "Epoch 13/20\n",
      " - 1s - loss: 10.6065 - categorical_accuracy: 0.3182 - val_loss: 6.4912 - val_categorical_accuracy: 0.5823\n",
      "Epoch 14/20\n",
      " - 1s - loss: 10.6675 - categorical_accuracy: 0.3158 - val_loss: 8.0933 - val_categorical_accuracy: 0.4869\n",
      "Epoch 15/20\n",
      " - 1s - loss: 10.6518 - categorical_accuracy: 0.3116 - val_loss: 9.9734 - val_categorical_accuracy: 0.3556\n",
      "Epoch 16/20\n",
      " - 1s - loss: 11.0383 - categorical_accuracy: 0.2964 - val_loss: 6.7412 - val_categorical_accuracy: 0.3222\n",
      "Epoch 17/20\n",
      " - 1s - loss: 11.2522 - categorical_accuracy: 0.2837 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 18/20\n",
      " - 1s - loss: 11.2576 - categorical_accuracy: 0.2760 - val_loss: 7.4447 - val_categorical_accuracy: 0.4558\n",
      "Epoch 19/20\n",
      " - 1s - loss: 11.1426 - categorical_accuracy: 0.2826 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "Epoch 20/20\n",
      " - 1s - loss: 11.5705 - categorical_accuracy: 0.2673 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "End training\n",
      "accuracy =  0.256 time_taken =  17.727\n",
      "Start training\n",
      "Train on 3768 samples, validate on 419 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 11.1011 - categorical_accuracy: 0.2938 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 2/20\n",
      " - 1s - loss: 11.4402 - categorical_accuracy: 0.2803 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 3/20\n",
      " - 1s - loss: 11.3263 - categorical_accuracy: 0.2792 - val_loss: 10.1012 - val_categorical_accuracy: 0.3723\n",
      "Epoch 4/20\n",
      " - 1s - loss: 11.1844 - categorical_accuracy: 0.2885 - val_loss: 8.5235 - val_categorical_accuracy: 0.4105\n",
      "Epoch 5/20\n",
      " - 1s - loss: 10.8104 - categorical_accuracy: 0.3110 - val_loss: 6.7498 - val_categorical_accuracy: 0.5131\n",
      "Epoch 6/20\n",
      " - 1s - loss: 11.0714 - categorical_accuracy: 0.2954 - val_loss: 11.2672 - val_categorical_accuracy: 0.3007\n",
      "Epoch 7/20\n",
      " - 1s - loss: 10.8824 - categorical_accuracy: 0.3068 - val_loss: 9.7452 - val_categorical_accuracy: 0.3819\n",
      "Epoch 8/20\n",
      " - 1s - loss: 10.8981 - categorical_accuracy: 0.3057 - val_loss: 9.1211 - val_categorical_accuracy: 0.4153\n",
      "Epoch 9/20\n",
      " - 1s - loss: 10.8071 - categorical_accuracy: 0.3102 - val_loss: 6.4669 - val_categorical_accuracy: 0.5823\n",
      "Epoch 10/20\n",
      " - 1s - loss: 10.7078 - categorical_accuracy: 0.3081 - val_loss: 8.8549 - val_categorical_accuracy: 0.4057\n",
      "Epoch 11/20\n",
      " - 1s - loss: 10.9572 - categorical_accuracy: 0.2922 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 12/20\n",
      " - 1s - loss: 11.2415 - categorical_accuracy: 0.2858 - val_loss: 11.2119 - val_categorical_accuracy: 0.3007\n",
      "Epoch 13/20\n",
      " - 1s - loss: 11.2164 - categorical_accuracy: 0.2755 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 14/20\n",
      " - 1s - loss: 11.7021 - categorical_accuracy: 0.2718 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 15/20\n",
      " - 1s - loss: 11.6732 - categorical_accuracy: 0.2718 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 16/20\n",
      " - 1s - loss: 11.6358 - categorical_accuracy: 0.2749 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 17/20\n",
      " - 1s - loss: 11.6051 - categorical_accuracy: 0.2723 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 18/20\n",
      " - 1s - loss: 11.1392 - categorical_accuracy: 0.2861 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "Epoch 19/20\n",
      " - 1s - loss: 11.4853 - categorical_accuracy: 0.2723 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 20/20\n",
      " - 1s - loss: 11.4636 - categorical_accuracy: 0.2755 - val_loss: 7.9897 - val_categorical_accuracy: 0.3007\n",
      "End training\n",
      "accuracy =  0.273 time_taken =  17.279\n",
      "Start training\n",
      "Train on 3768 samples, validate on 419 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 11.3064 - categorical_accuracy: 0.2797 - val_loss: 7.6082 - val_categorical_accuracy: 0.4535\n",
      "Epoch 2/20\n",
      " - 1s - loss: 11.0587 - categorical_accuracy: 0.2956 - val_loss: 5.7488 - val_categorical_accuracy: 0.6014\n",
      "Epoch 3/20\n",
      " - 1s - loss: 11.4616 - categorical_accuracy: 0.2712 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 4/20\n",
      " - 1s - loss: 11.4265 - categorical_accuracy: 0.2800 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 5/20\n",
      " - 1s - loss: 11.5088 - categorical_accuracy: 0.2712 - val_loss: 5.7275 - val_categorical_accuracy: 0.3079\n",
      "Epoch 6/20\n",
      " - 1s - loss: 11.4909 - categorical_accuracy: 0.2747 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "Epoch 7/20\n",
      " - 1s - loss: 11.6300 - categorical_accuracy: 0.2566 - val_loss: 7.9217 - val_categorical_accuracy: 0.3007\n",
      "Epoch 8/20\n",
      " - 1s - loss: 11.6757 - categorical_accuracy: 0.2540 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "Epoch 9/20\n",
      " - 1s - loss: 11.9159 - categorical_accuracy: 0.2484 - val_loss: 10.1171 - val_categorical_accuracy: 0.3723\n",
      "Epoch 10/20\n",
      " - 1s - loss: 11.5976 - categorical_accuracy: 0.2686 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 11/20\n",
      " - 1s - loss: 11.5826 - categorical_accuracy: 0.2622 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 12/20\n",
      " - 1s - loss: 11.5417 - categorical_accuracy: 0.2718 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 13/20\n",
      " - 1s - loss: 11.7139 - categorical_accuracy: 0.2686 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 14/20\n",
      " - 1s - loss: 11.5598 - categorical_accuracy: 0.2667 - val_loss: 8.7341 - val_categorical_accuracy: 0.3007\n",
      "Epoch 15/20\n",
      " - 1s - loss: 11.7789 - categorical_accuracy: 0.2633 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 16/20\n",
      " - 1s - loss: 11.7319 - categorical_accuracy: 0.2694 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 17/20\n",
      " - 1s - loss: 11.6476 - categorical_accuracy: 0.2728 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 18/20\n",
      " - 1s - loss: 11.5132 - categorical_accuracy: 0.2710 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 19/20\n",
      " - 1s - loss: 11.6556 - categorical_accuracy: 0.2707 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 20/20\n",
      " - 1s - loss: 11.6920 - categorical_accuracy: 0.2686 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "End training\n",
      "accuracy =  0.273 time_taken =  17.162\n",
      "Start training\n",
      "Train on 3769 samples, validate on 419 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 11.6974 - categorical_accuracy: 0.2725 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 2/20\n",
      " - 1s - loss: 11.6304 - categorical_accuracy: 0.2685 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 3/20\n",
      " - 1s - loss: 11.5806 - categorical_accuracy: 0.2669 - val_loss: 10.1556 - val_categorical_accuracy: 0.3699\n",
      "Epoch 4/20\n",
      " - 1s - loss: 11.8752 - categorical_accuracy: 0.2502 - val_loss: 10.1556 - val_categorical_accuracy: 0.3699\n",
      "Epoch 5/20\n",
      " - 1s - loss: 11.7138 - categorical_accuracy: 0.2550 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 6/20\n",
      " - 1s - loss: 11.6607 - categorical_accuracy: 0.2592 - val_loss: 5.8970 - val_categorical_accuracy: 0.4368\n",
      "Epoch 7/20\n",
      " - 1s - loss: 11.9862 - categorical_accuracy: 0.2412 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 8/20\n",
      " - 1s - loss: 11.7313 - categorical_accuracy: 0.2698 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 9/20\n",
      " - 1s - loss: 11.7525 - categorical_accuracy: 0.2698 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 10/20\n",
      " - 1s - loss: 11.7698 - categorical_accuracy: 0.2693 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 11/20\n",
      " - 1s - loss: 11.7334 - categorical_accuracy: 0.2709 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 12/20\n",
      " - 1s - loss: 11.7360 - categorical_accuracy: 0.2701 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      " - 1s - loss: 11.7489 - categorical_accuracy: 0.2666 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 14/20\n",
      " - 1s - loss: 11.6994 - categorical_accuracy: 0.2600 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 15/20\n",
      " - 1s - loss: 11.5626 - categorical_accuracy: 0.2648 - val_loss: 10.1556 - val_categorical_accuracy: 0.3699\n",
      "Epoch 16/20\n",
      " - 1s - loss: 11.6089 - categorical_accuracy: 0.2613 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 17/20\n",
      " - 1s - loss: 11.7112 - categorical_accuracy: 0.2632 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 18/20\n",
      " - 1s - loss: 11.6100 - categorical_accuracy: 0.2706 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 19/20\n",
      " - 1s - loss: 11.7102 - categorical_accuracy: 0.2571 - val_loss: 10.1556 - val_categorical_accuracy: 0.3699\n",
      "Epoch 20/20\n",
      " - 1s - loss: 11.9001 - categorical_accuracy: 0.2523 - val_loss: 10.1556 - val_categorical_accuracy: 0.3699\n",
      "End training\n",
      "accuracy =  0.256 time_taken =  17.338\n",
      "Start training\n",
      "Train on 3770 samples, validate on 419 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 11.6748 - categorical_accuracy: 0.2660 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 2/20\n",
      " - 1s - loss: 11.7214 - categorical_accuracy: 0.2695 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 3/20\n",
      " - 1s - loss: 11.7396 - categorical_accuracy: 0.2639 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 4/20\n",
      " - 1s - loss: 11.7722 - categorical_accuracy: 0.2647 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 5/20\n",
      " - 1s - loss: 11.7206 - categorical_accuracy: 0.2679 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 6/20\n",
      " - 1s - loss: 11.6479 - categorical_accuracy: 0.2655 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 7/20\n",
      " - 1s - loss: 11.4162 - categorical_accuracy: 0.2761 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 8/20\n",
      " - 1s - loss: 11.6584 - categorical_accuracy: 0.2679 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 9/20\n",
      " - 1s - loss: 11.7584 - categorical_accuracy: 0.2692 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 10/20\n",
      " - 1s - loss: 11.7538 - categorical_accuracy: 0.2698 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 11/20\n",
      " - 1s - loss: 11.7561 - categorical_accuracy: 0.2700 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 12/20\n",
      " - 1s - loss: 11.7548 - categorical_accuracy: 0.2703 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 13/20\n",
      " - 1s - loss: 11.7180 - categorical_accuracy: 0.2719 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 14/20\n",
      " - 1s - loss: 11.7552 - categorical_accuracy: 0.2706 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 15/20\n",
      " - 1s - loss: 11.7691 - categorical_accuracy: 0.2690 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 16/20\n",
      " - 1s - loss: 11.7447 - categorical_accuracy: 0.2706 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 17/20\n",
      " - 1s - loss: 11.6344 - categorical_accuracy: 0.2724 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 18/20\n",
      " - 1s - loss: 11.6006 - categorical_accuracy: 0.2642 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 19/20\n",
      " - 1s - loss: 11.7407 - categorical_accuracy: 0.2682 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 20/20\n",
      " - 1s - loss: 11.6113 - categorical_accuracy: 0.2610 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "End training\n",
      "accuracy =  0.274 time_taken =  16.959\n",
      "Start training\n",
      "Train on 3771 samples, validate on 419 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 11.7474 - categorical_accuracy: 0.2620 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 2/20\n",
      " - 1s - loss: 11.6960 - categorical_accuracy: 0.2652 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 3/20\n",
      " - 1s - loss: 11.6966 - categorical_accuracy: 0.2647 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 4/20\n",
      " - 1s - loss: 11.7764 - categorical_accuracy: 0.2676 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 5/20\n",
      " - 1s - loss: 11.5888 - categorical_accuracy: 0.2729 - val_loss: 6.9900 - val_categorical_accuracy: 0.4129\n",
      "Epoch 6/20\n",
      " - 1s - loss: 11.8856 - categorical_accuracy: 0.2498 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 7/20\n",
      " - 1s - loss: 11.5974 - categorical_accuracy: 0.2678 - val_loss: 8.4779 - val_categorical_accuracy: 0.3007\n",
      "Epoch 8/20\n",
      " - 1s - loss: 11.4437 - categorical_accuracy: 0.2739 - val_loss: 8.1306 - val_categorical_accuracy: 0.3007\n",
      "Epoch 9/20\n",
      " - 1s - loss: 11.4220 - categorical_accuracy: 0.2745 - val_loss: 11.2461 - val_categorical_accuracy: 0.3007\n",
      "Epoch 10/20\n",
      " - 1s - loss: 11.5961 - categorical_accuracy: 0.2633 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 11/20\n",
      " - 1s - loss: 11.5967 - categorical_accuracy: 0.2705 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 12/20\n",
      " - 1s - loss: 11.6419 - categorical_accuracy: 0.2729 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 13/20\n",
      " - 1s - loss: 11.7191 - categorical_accuracy: 0.2697 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 14/20\n",
      " - 1s - loss: 11.7431 - categorical_accuracy: 0.2668 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 15/20\n",
      " - 1s - loss: 11.5292 - categorical_accuracy: 0.2737 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 16/20\n",
      " - 1s - loss: 11.4777 - categorical_accuracy: 0.2705 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 17/20\n",
      " - 1s - loss: 11.3955 - categorical_accuracy: 0.2792 - val_loss: 6.3857 - val_categorical_accuracy: 0.4893\n",
      "Epoch 18/20\n",
      " - 1s - loss: 11.3413 - categorical_accuracy: 0.2787 - val_loss: 6.7858 - val_categorical_accuracy: 0.3795\n",
      "Epoch 19/20\n",
      " - 1s - loss: 11.4077 - categorical_accuracy: 0.2787 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "Epoch 20/20\n",
      " - 1s - loss: 11.4355 - categorical_accuracy: 0.2800 - val_loss: 11.2711 - val_categorical_accuracy: 0.3007\n",
      "End training\n",
      "accuracy =  0.275 time_taken =  16.559\n",
      "Start training\n",
      "Train on 3772 samples, validate on 420 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 11.4094 - categorical_accuracy: 0.2853 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 2/20\n",
      " - 1s - loss: 11.5934 - categorical_accuracy: 0.2733 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 3/20\n",
      " - 1s - loss: 11.5324 - categorical_accuracy: 0.2715 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 4/20\n",
      " - 1s - loss: 11.6273 - categorical_accuracy: 0.2654 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 5/20\n",
      " - 1s - loss: 11.4749 - categorical_accuracy: 0.2760 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 6/20\n",
      " - 1s - loss: 11.6246 - categorical_accuracy: 0.2717 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 7/20\n",
      " - 1s - loss: 11.5062 - categorical_accuracy: 0.2733 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 8/20\n",
      " - 1s - loss: 11.5784 - categorical_accuracy: 0.2686 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 9/20\n",
      " - 1s - loss: 11.6036 - categorical_accuracy: 0.2699 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 10/20\n",
      " - 1s - loss: 11.6695 - categorical_accuracy: 0.2585 - val_loss: 7.7843 - val_categorical_accuracy: 0.4214\n",
      "Epoch 11/20\n",
      " - 1s - loss: 11.4010 - categorical_accuracy: 0.2778 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 12/20\n",
      " - 1s - loss: 11.3814 - categorical_accuracy: 0.2784 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 13/20\n",
      " - 1s - loss: 11.3868 - categorical_accuracy: 0.2839 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 14/20\n",
      " - 1s - loss: 11.4206 - categorical_accuracy: 0.2723 - val_loss: 8.5301 - val_categorical_accuracy: 0.2357\n",
      "Epoch 15/20\n",
      " - 1s - loss: 11.2089 - categorical_accuracy: 0.2871 - val_loss: 12.2876 - val_categorical_accuracy: 0.2357\n",
      "Epoch 16/20\n",
      " - 1s - loss: 11.4182 - categorical_accuracy: 0.2805 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "Epoch 17/20\n",
      " - 1s - loss: 11.7131 - categorical_accuracy: 0.2609 - val_loss: 9.3255 - val_categorical_accuracy: 0.4214\n",
      "Epoch 18/20\n",
      " - 1s - loss: 11.5434 - categorical_accuracy: 0.2680 - val_loss: 6.1587 - val_categorical_accuracy: 0.5714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      " - 1s - loss: 11.6059 - categorical_accuracy: 0.2641 - val_loss: 6.6464 - val_categorical_accuracy: 0.5786\n",
      "Epoch 20/20\n",
      " - 1s - loss: 11.5719 - categorical_accuracy: 0.2569 - val_loss: 12.3188 - val_categorical_accuracy: 0.2357\n",
      "End training\n",
      "accuracy =  0.276 time_taken =  16.53\n",
      "Accuracy Final =  0.303 Time_taken average =  17.596\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-718374eed873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mavg_records\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'avg_records.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mfd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mname_of_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mlp_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_hid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_ikea.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_of_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not list"
     ]
    }
   ],
   "source": [
    "for i in range(200,201):\n",
    "    print('number of hidden nodes = ', i)\n",
    "    n_hid = i\n",
    "    lr = 0.1\n",
    "    num_splits = 10\n",
    "    mlp_model = initialize_nn(frame_size, n_hid, lr)\n",
    "    skf = StratifiedKFold(n_splits=num_splits)\n",
    "    avg_acc = 0\n",
    "    avg_time = 0\n",
    "    for train_index, test_index in skf.split(features, new_labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        Y_train, Y_test = new_labels[train_index], new_labels[test_index]\n",
    "        Y_train = to_categorical(Y_train, num_classes=12, dtype='float32')\n",
    "        Y_test = to_categorical(Y_test, num_classes=12, dtype='float32')\n",
    "        print('Start training')\n",
    "        start = time.time()\n",
    "        mlp_model.fit(X_train, Y_train, validation_split=0.1, batch_size=32, epochs=20, verbose=2, shuffle=True)\n",
    "        end = time.time()\n",
    "        print('End training')\n",
    "        predictions = mlp_model.predict(X_test)\n",
    "        predictions = (predictions>0.5)\n",
    "        time_taken = end-start\n",
    "        score = accuracy_score(Y_test, predictions)\n",
    "        print('accuracy = ', float('{0:.3f}'.format(score)), 'time_taken = ', float('{0:.3f}'.format(time_taken)))\n",
    "        avg_acc+=score\n",
    "        avg_time+=time_taken\n",
    "        records.append([n_hid, lr, float('{0:.3f}'.format(score)), float('{0:.3f}'.format(time_taken))])\n",
    "    avg_acc /= num_splits\n",
    "    avg_time /= num_splits\n",
    "    print('Accuracy Final = ', float('{0:.3f}'.format(avg_acc)), 'Time_taken average = ', float('{0:.3f}'.format(avg_time)))\n",
    "    temp = [n_hid, lr, float('{0:.3f}'.format(avg_acc)), float('{0:.3f}'.format(avg_time))]\n",
    "    avg_records.append(temp)\n",
    "    with open('avg_records.csv', 'a') as fd:\n",
    "        fd.write(temp)\n",
    "    name_of_file = 'mlp_'+str(n_hid)+'_ikea.pkl'\n",
    "    with open(name_of_file, 'wb') as fid:\n",
    "        pickle.dump(mlp_model, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data = pd.DataFrame(avg_records)\n",
    "r_data.to_csv('avg_records_mlp.csv', header=['n_hid', 'learn_rate', 'accuracy', 'time_taken'], index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.034</td>\n",
       "      <td>6.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.024</td>\n",
       "      <td>6.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.140</td>\n",
       "      <td>6.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.114</td>\n",
       "      <td>6.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.065</td>\n",
       "      <td>5.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.019</td>\n",
       "      <td>5.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.096</td>\n",
       "      <td>5.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.045</td>\n",
       "      <td>5.931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.159</td>\n",
       "      <td>6.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.026</td>\n",
       "      <td>6.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>6.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.158</td>\n",
       "      <td>6.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.180</td>\n",
       "      <td>5.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.226</td>\n",
       "      <td>6.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.218</td>\n",
       "      <td>5.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.222</td>\n",
       "      <td>5.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.146</td>\n",
       "      <td>5.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.185</td>\n",
       "      <td>5.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.153</td>\n",
       "      <td>5.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.225</td>\n",
       "      <td>5.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.203</td>\n",
       "      <td>5.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.220</td>\n",
       "      <td>5.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.242</td>\n",
       "      <td>9.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.216</td>\n",
       "      <td>6.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.188</td>\n",
       "      <td>6.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.494</td>\n",
       "      <td>12.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.665</td>\n",
       "      <td>8.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.613</td>\n",
       "      <td>8.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.662</td>\n",
       "      <td>8.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.682</td>\n",
       "      <td>8.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.688</td>\n",
       "      <td>9.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.631</td>\n",
       "      <td>9.551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.646</td>\n",
       "      <td>8.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.684</td>\n",
       "      <td>8.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.674</td>\n",
       "      <td>8.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.460</td>\n",
       "      <td>13.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.658</td>\n",
       "      <td>9.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.624</td>\n",
       "      <td>8.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.699</td>\n",
       "      <td>8.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.660</td>\n",
       "      <td>8.634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.705</td>\n",
       "      <td>8.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.640</td>\n",
       "      <td>8.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.592</td>\n",
       "      <td>8.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.636</td>\n",
       "      <td>8.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.685</td>\n",
       "      <td>8.505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.549</td>\n",
       "      <td>17.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.690</td>\n",
       "      <td>12.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.759</td>\n",
       "      <td>13.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.738</td>\n",
       "      <td>12.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.699</td>\n",
       "      <td>12.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.725</td>\n",
       "      <td>12.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.631</td>\n",
       "      <td>12.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.644</td>\n",
       "      <td>12.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589</td>\n",
       "      <td>12.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.607</td>\n",
       "      <td>12.784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2      3       4\n",
       "0      1    1  0.1  0.000   9.090\n",
       "1      1    1  0.1  0.034   6.116\n",
       "2      1    1  0.1  0.024   6.094\n",
       "3      1    1  0.1  0.140   6.254\n",
       "4      1    1  0.1  0.114   6.118\n",
       "5      1    1  0.1  0.065   5.897\n",
       "6      1    1  0.1  0.019   5.746\n",
       "7      1    1  0.1  0.000   9.644\n",
       "8      1    1  0.1  0.002   6.093\n",
       "9      1    1  0.1  0.096   5.890\n",
       "10     1    1  0.1  0.000   6.161\n",
       "11     1    1  0.1  0.045   5.931\n",
       "12     1    1  0.1  0.159   6.014\n",
       "13     1    1  0.1  0.026   6.009\n",
       "14     1    1  0.1  0.175   6.308\n",
       "15     1    1  0.1  0.158   6.078\n",
       "16     1    1  0.1  0.180   5.952\n",
       "17     2    2  0.1  0.000   9.962\n",
       "18     2    2  0.1  0.226   6.004\n",
       "19     2    2  0.1  0.218   5.859\n",
       "20     2    2  0.1  0.222   5.801\n",
       "21     2    2  0.1  0.146   5.866\n",
       "22     2    2  0.1  0.185   5.851\n",
       "23     2    2  0.1  0.153   5.810\n",
       "24     2    2  0.1  0.225   5.896\n",
       "25     2    2  0.1  0.203   5.846\n",
       "26     2    2  0.1  0.220   5.802\n",
       "27     3    3  0.1  0.242   9.879\n",
       "28     3    3  0.1  0.216   6.135\n",
       "29     3    3  0.1  0.188   6.075\n",
       "..   ...  ...  ...    ...     ...\n",
       "177   18   18  0.1  0.494  12.704\n",
       "178   18   18  0.1  0.665   8.440\n",
       "179   18   18  0.1  0.613   8.629\n",
       "180   18   18  0.1  0.662   8.598\n",
       "181   18   18  0.1  0.682   8.743\n",
       "182   18   18  0.1  0.688   9.476\n",
       "183   18   18  0.1  0.631   9.551\n",
       "184   18   18  0.1  0.646   8.883\n",
       "185   18   18  0.1  0.684   8.436\n",
       "186   18   18  0.1  0.674   8.651\n",
       "187   19   19  0.1  0.460  13.260\n",
       "188   19   19  0.1  0.658   9.021\n",
       "189   19   19  0.1  0.624   8.666\n",
       "190   19   19  0.1  0.699   8.533\n",
       "191   19   19  0.1  0.660   8.634\n",
       "192   19   19  0.1  0.705   8.833\n",
       "193   19   19  0.1  0.640   8.727\n",
       "194   19   19  0.1  0.592   8.506\n",
       "195   19   19  0.1  0.636   8.498\n",
       "196   19   19  0.1  0.685   8.505\n",
       "197  100  100  0.1  0.549  17.817\n",
       "198  100  100  0.1  0.690  12.748\n",
       "199  100  100  0.1  0.759  13.001\n",
       "200  100  100  0.1  0.738  12.653\n",
       "201  100  100  0.1  0.699  12.565\n",
       "202  100  100  0.1  0.725  12.772\n",
       "203  100  100  0.1  0.631  12.670\n",
       "204  100  100  0.1  0.644  12.695\n",
       "205  100  100  0.1  0.589  12.828\n",
       "206  100  100  0.1  0.607  12.784\n",
       "\n",
       "[207 rows x 5 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
